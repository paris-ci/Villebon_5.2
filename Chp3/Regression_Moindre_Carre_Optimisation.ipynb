{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel du modèle linéaire <a id='model'></a> \n",
    "\n",
    "$$y \\sim f_{\\theta}(x)$$\n",
    "\n",
    "avec comme fonction f une forme linéaire\n",
    "\n",
    "$$f_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 +\\epsilon $$\n",
    "\n",
    "\n",
    "### Hypothèses du modèle linéaire statistique\n",
    "\n",
    "* les erreurs suivent une loi normale de moyenne nulle\n",
    "* la variance est la même pour tous (homoscédasticité) : la variance est la même pour l'ensemble des termes d'erreures gaussiens\n",
    "$Var(\\epsilon_i)=\\sigma$\n",
    "* les termes d'erreurs pour les différents $x_i$ sont indépendants les uns des autres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commençons par définir quelques fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, params=(0,1), var=1):\n",
    "    \"\"\"Generate a linear function f(x)=a*x+b+N(0,1)\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.array()) : vector used to generate the output\n",
    "        params (tuple of size 2) : b=params[0] and a=params[1]\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array()\n",
    "    \"\"\"\n",
    "    a, b = params[1], params[0]\n",
    "    return a*x + b + var*np.random.normal(size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 10*np.random.random(50)\n",
    "x.sort()\n",
    "my_params = (0,10)\n",
    "y = linear(x, my_params, var=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode des moindres carrés\n",
    "\n",
    "Afin de fixer les coefficients, nous définissons une mesure qui nous permet de connaître de façon empirique la qualité de l'ajustement.\n",
    "\n",
    "\n",
    "Quelques constats :\n",
    "1. Aucune droite ne peut passer par tous les points, vu qu'ils ne sont pas alignés\n",
    "1. Chaque droite passe plus près de certains que d'autres\n",
    "1. Nous décrétons pour le moment que la meilleure droite est celle qui passe le plus près de l'ensemble des points\n",
    "\n",
    "Pour chaque x, nous allons calculer la distance cumulée entre $y_i$ projeté sur la droite à l'abscisse $x_i$, et minimiser cette quantité\n",
    "\n",
    "$$Erreur(\\theta) = \\frac{1}{N}\\sum_i (y_i-f_{\\theta}(x_i))^2$$\n",
    "\n",
    "* Les erreurs s'ajoutent toujours\n",
    "* Pénalité sur les déviations les plus larges\n",
    "* La fonction carré est dérivable\n",
    "\n",
    "Nous souhaitons $\\hat \\theta$ tq $Erreur(\\theta) \\ge Erreur(\\hat \\theta)$\n",
    "\n",
    "#### Résolution mathématique\n",
    "\n",
    "Nous souhaitons minimiser le terme d'erreur, et donc trouver pour quelles valeurs de $\\theta$ la dérivée s'annule\n",
    "$$\\frac{\\partial E}{\\partial \\theta_i} = 0$$\n",
    "\n",
    "pour la fonction $f_{\\theta}(x) = \\theta_0 + \\theta_1 x +\\epsilon $\n",
    "\n",
    "On obtient les équations suivantes pour les paramètres (un très bon exercice à faire) \n",
    "\n",
    "$$\\sum_i x_i (y_i-\\theta_0-\\theta_1 x_i) = 0  $$\n",
    "$$\\sum_i (y_i-\\theta_0-\\theta_1 x_i) = 0  $$\n",
    "\n",
    "\n",
    "Il est possible dans le cadre de ce modèle d'avoir la solution exacte :\n",
    "\n",
    "$$\\hat  \\theta_1 = \\frac{\\sum(x_i-\\bar x)(y_i-\\bar y)}{\\sum (x_i - \\bar x)^2} = \\frac{cov(x,y)}{var(x)}$$\n",
    "$$\\hat \\theta_0 = \\bar y - \\hat \\theta_1 \\bar x$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_hypothesis(x, params=(0,1)):\n",
    "    \"\"\"Linear function f(x)=a*x+b\n",
    "    \n",
    "    Args:\n",
    "        x (numpy.array()) : vector used to generate the output\n",
    "        params (tuple of size 2) : b=params[0] and a=params[1]\n",
    "    \n",
    "    Returns:\n",
    "        numpy.array()\n",
    "    \"\"\"\n",
    "    a, b = params[1], params[0]\n",
    "    return b + a*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(x, y, func, func_args):\n",
    "    \"\"\"Cost function associated with (x,y) couple and function to fit\n",
    "    \n",
    "    Args:\n",
    "        x : feature vector \n",
    "        y : explained variable\n",
    "        func : target function\n",
    "        func_args : arguments of function\n",
    "        \n",
    "    Returns:\n",
    "        scalar value of cost\n",
    "    \"\"\"\n",
    "    return 1./len(x)*sum(np.square(y-func(x,func_args)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(x,y,linear_hypothesis, (5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(x,y,linear_hypothesis, (0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction n'est pas la plus générale que l'on puisse écrire. Elle suppose en effet que la fonction cible ne prend qu'un seul argument. Il existe des façons en python de définir des fonctions qui prennent un nombre quelconque d'argument. Vous pouvez voir ici par exemple pour un exemple simple et explicite : http://stackoverflow.com/questions/36901/what-does-double-star-and-star-do-for-python-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recherche exhaustive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def params_search():\n",
    "    \"\"\"2d scan for possible values and return best ones\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    params = []\n",
    "    for p0 in np.linspace(0,10):\n",
    "        for p1 in np.linspace(0,10):\n",
    "            p = (p0,p1)\n",
    "            #print p\n",
    "            c = cost(x, y, linear_hypothesis, p)\n",
    "            errors.append(c)\n",
    "            params.append(p)\n",
    "    return params[errors.index(min(errors))]\n",
    "\n",
    "best = params_search()\n",
    "print(\"Paramètres optimisant les moindres carrés :\", best)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,best[0]+best[1]*x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recherche aléatoire de paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_params_search():\n",
    "    errors = []\n",
    "    params = []\n",
    "    #choix arbitraire de recherche entre 0 et 10\n",
    "    for p in 10*np.random.rand(100,2):\n",
    "        p = tuple(p) \n",
    "        c = cost(x,y, linear_hypothesis, p)\n",
    "        errors.append(c)\n",
    "        params.append(p)\n",
    "    return params[errors.index(min(errors))]\n",
    "\n",
    "best = random_params_search()\n",
    "print(\"Paramètres optimisant les moindres carrés :\", best)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,best[0]+best[1]*x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut chercher longtemps, surtout si l'on n'a pas une bonne idée des valeurs des paramètres que l'on cherche.\n",
    "\n",
    "Les problèmes de recherche d'optimum et de racines de fonction sont particulièrement courants. De nombreuses méthodes existent, qui dépassent le cadre de ce cours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de gradient\n",
    "\n",
    "On démarre avec des paramètres $\\theta_i$ aléatoires.\n",
    "\n",
    "Plutôt que de tester de nouvelles combinaisons au hasard, nous allons mettre à jour les valeurs que nous avons en ajoutant ou soustrayant une certaine quantité dans la direction qui minimise l'erreur.\n",
    "\n",
    "$$\\theta_{i+1} = \\theta_i + \\delta$$\n",
    "\n",
    "Si Erreur croît, alors sa dérivée est positive, et si elle décroit, alors elle est négative. Nous voulons donc aller dans la direction des dérivées décroissantes.\n",
    "\n",
    "$\\delta \\propto -\\frac{\\partial E}{\\partial \\theta_i}$\n",
    "\n",
    "$$\\theta_{i+1} \\leftarrow \\theta_i + - \\alpha \\frac{\\partial E}{\\partial \\theta_i}$$\n",
    "<img src=\"img/2000px-Gradient_descent.svg.png\" width=\"500\">\n",
    "\n",
    "Le gradient vient de la forme vectorielle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, params, alpha=0.01, iterations=1000):\n",
    "    params = list(params)\n",
    "    N=len(y)\n",
    "    error_history = np.array([]).reshape(0,3)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        params[0] = params[0]-(alpha/N)*sum(linear_hypothesis(x,params)-y)\n",
    "        params[1] = params[1]-(alpha/N)*(linear_hypothesis(x,params)-y).dot(x)\n",
    "        error_history = np.vstack([error_history,[cost(x,y, linear, tuple(params)), params[0], params[1]]])\n",
    "    return params, error_history\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, error_history = gradient_descent(x, y, (5,10), alpha=0.001, iterations=10000)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions maintenant notre regression, l'évolution de l'erreur ainsi que des paramètres pendant la descente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.plot(x, p[0]+p[1]*x, 'g')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(50), error_history[:50,0])\n",
    "plt.title(\"Erreur\")\n",
    "plt.xlabel(\"itération\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(error_history[:,1], label='b')\n",
    "plt.plot(error_history[:,2], label='a')\n",
    "\n",
    "plt.xlabel(\"itération\")\n",
    "plt.ylabel(\"valeur\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle différence y'a-t-il dans les résultats entre `alpha=0.01` et `alpha=0.001` ?   \n",
    "Comment peut-on améliorer notre code ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour en savoir plus sur la descente de gradient, qui joue aujourd'hui un rôle central dans le deep learning, vous pouvez consulter les liens suivants :\n",
    "- http://ruder.io/optimizing-gradient-descent/\n",
    "- https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également utiliser les bibliothèques disponibles sous python.\n",
    "\n",
    "Par exemple avec scipy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "def func(x, a, b):\n",
    "    return a*x + b\n",
    "\n",
    "popt, pcov = curve_fit(func, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais il existe également une fonction spécifique pour la regression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction renvoie notamment le coefficient de détermination\n",
    "\n",
    "$$R^2 = 1-\\frac{\\sum_i ( y_i - \\hat y_i )^2}{\\sum_i (y_i - \\bar y_i )^2}$$\n",
    "\n",
    "qui varie entre 0 et 1, et donne une indication sur le degré d'adéquation entre le modèle linéaire et les données, 1 voulant dire que le modèle capture 100% des variations. \n",
    "\n",
    "<img src=\"img/600px-Coefficient_of_Determination.svg.png\"></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour en savoir, google est votre ami, mais cette vidéo peut également être utile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"lng4ZgConCM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également commun de faire un test statistique sur le résultat de la régression, avec comme hypothèse que les coefficients sont nuls.\n",
    "\n",
    "La valeur p est la probabilité que la variable slope ait la valeur déduite des observations si ces dernières sont le résultat de l'hypothèse nulle, soit ici qu'il n'y ait pas de dépendance linéaire. \n",
    "\n",
    "On constate dans notre cas que celle-ci est très faible, et on peut donc rejeter l'hypothèse nulle.\n",
    "\n",
    "Généralement, une valeur critique pour p est 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que se passe-t-il si on les erreurs deviennent très importantes ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy et Scipy sont des bilbiothèques dédiées à l'analyse numérique, et à la manipulation de vecteurs et de matrices, ainsi que la résolution de problèmes liés.\n",
    "\n",
    "Des bibliothèques plus haut niveau (qui reposent notamment sur Numpy), sont plus spécialement dédiées à l'analyse et au mining de données.\n",
    "\n",
    "* [statsmodel](http://statsmodels.sourceforge.net/), qui est orienté analyse statistique\n",
    "* [scikit-learn](http://scikit-learn.org/stable/), qui est une bibliothèque de machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résolution analytique pour les coefficients de la régression linéaire\n",
    "\n",
    "$$\\hat  \\theta_1 = \\frac{\\sum(x_i-\\bar x)(y_i-\\bar y)}{\\sum (x_i - \\bar x)^2} = \\frac{cov(x,y)}{var(x)}$$\n",
    "$$\\hat \\theta_0 = \\bar y - \\hat \\theta_1 \\bar x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moyenne(x):\n",
    "    pass\n",
    "\n",
    "def produit_scalaire(x,y):\n",
    "    pass\n",
    "\n",
    "def variance(x):\n",
    "    pass \n",
    "\n",
    "def covariance(x,y):\n",
    "    pass \n",
    "    \n",
    "def linear_parameters(x,y):\n",
    "    pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
